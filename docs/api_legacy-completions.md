# Legacy completions API

Version: 1.30

On this page

Use the Privatemode completions API to generate text from a prompt via a large language model. The API is compatible with the [legacy OpenAI Completions API](https://platform.openai.com/docs/api-reference/completions/create).
It's deprecated in favor of the [chat completions API](/api/chat-completions).
To generate text, send your requests to the [Privatemode proxy](/guides/proxy-configuration). Chat requests and responses are encrypted, both in transit and during processing.

## Example prompting[​](#example-prompting "Direct link to Example prompting")

For prompting, use the following proxy endpoint:

```bash
POST /v1/completions
```

This endpoint generates a response to a chat prompt.

### Request body[​](#request-body "Direct link to Request body")

* `model` string: The name of a currently [available model](/models/overview). Note that models are updated regularly, and support for older models is discontinued over time. Use `GET /v1/models` to get a list of available models as described in the [models API](/api/models#list-models). Model name `latest` is deprecated and will be removed in a future update.
* `prompt` string or list: The prompts for which a response is generated.
* Additional parameters: These mirror the OpenAI API and are supported based on the model server's capabilities.

### Returns[​](#returns "Direct link to Returns")

The response is a [completion object](https://platform.openai.com/docs/api-reference/completions/object) or a sequence of completion objects containing:

* `choices` list: The response generated by the model.
* Other parameters: Other fields are consistent with the OpenAI API specifications.

**Example request**

```bash
#!/usr/bin/env bash  
  
curl localhost:8080/v1/completions \  
  -H "Content-Type: application/json" \  
  -d '{  
    "model": "gpt-oss-120b",  
    "prompt": "Tell me a joke!"  
  }'
```

**Example response**

```bash
{  
  "id": "chat-6e8dc369b0614e2488df6a336c24c349",  
  "object": "text_completion",  
  "created": 1727968175,  
  "model": "gpt-oss-120b",  
  "choices": [  
    {  
      "index": 0,  
      "text": "What do you call a fake noodle?\n\nAn impasta.",  
      "logprobs": null,  
      "finish_reason": "stop",  
      "stop_reason": null  
    }  
  ],  
  "usage": {  
    "prompt_tokens": 40,  
    "total_tokens": 54,  
    "completion_tokens": 14  
  }  
}
```

## Prompt caching[​](#prompt-caching "Direct link to Prompt caching")

Privatemode supports prompt caching to reduce response latency when the first part of a prompt can be reused across requests. This is especially relevant for requests with long shared context or long conversation history.

Prompt caching is inactive by default. You can [enable it in the Privatemode proxy](/guides/proxy-configuration#prompt-caching). All requests sent via the same proxy share a cache and no further changes are required when making requests.

Alternatively, you can configure it per request via request field `cache_salt`, encoded as a string. All requests that use the same salt share a cache. For example, using the same salt in all requests of a user will create an isolated cache for that user.

* cURL
* Python

```bash
#!/usr/bin/env bash  
  
curl localhost:8080/v1/completions \  
  -H "Content-Type: application/json" \  
  -d '{  
    "model": "gpt-oss-120b",  
    "prompt": "Tell me a joke!",  
    "cache_salt": "Y3+y3nLYf3a0CvT7VtuI0W656YXyl0Rdvd8BHI9e2rU="  
  }'
```

```python
import openai  
import os  
  
client = openai.OpenAI(  
    api_key=os.environ.get("PRIVATE_MODE_API_KEY"), base_url="http://localhost:8080/v1"  
)  
  
client.completions.create(  
    model="gpt-oss-120b",  
    prompt="Tell me a joke!",  
    extra_body={  
        "cache_salt": "Y3+y3nLYf3a0CvT7VtuI0W656YXyl0Rdvd8BHI9e2rU=",  
    },  
)
```

When using the OpenAI Python client, provide `cache_salt` as part of an `extra_body` argument.
If caching is configured in both, the Privatemode proxy and in a request, the value from the request is used, allowing for more granular control by clients.

Cache salts should be kept private and have an entropy of at least 256 bits.
You can generate a secure salt with `openssl rand -base64 32`.

## Available chat completions models[​](#available-chat-completions-models "Direct link to Available chat completions models")

To list the available chat completions models, call the [`/v1/models` endpoint](/api/models) or see the [models overview](/models/overview).

* [Example prompting](#example-prompting)
  + [Request body](#request-body)
  + [Returns](#returns)
* [Prompt caching](#prompt-caching)
* [Available chat completions models](#available-chat-completions-models)